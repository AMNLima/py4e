Progragamas com a rede
=========================

Enquanto vários dos exemplos no livro estão focados em ler arquivos
e procurar dados nesses arquivos, existem várias fontes de informação diferentes
quando consideramos a Internet.

Neste capítulo, fingiremos ser um navegador da internet e acessaremos
as páginas da internet utilizando HyperText Transfer Protocol (HTTP). Então 
iremos os dados dessa página e utilizá-los.

HyperText Transfer Protocol - HTTP
------------------------------------

O protocolo de rede que provê a internet é atualmente simples e
existem suportes nativos em Python chamados `soquetes` que
facilita o trabalho de fazer conexões na rede e recuperar dados por meio
dos soquetes no programa de Python.

Um soquete parece com um arquivo, exceto que um simples soquete provê
uma conexão de via dupla entre dois programas. É possível ler e escrever
com o mesmo soquete. Se algo for escrito por um soquete, isso é enviado à 
aplicação que está do outro lado do soquete. Se algo for lido de um soquete,
os dados captados foram enviados pela outra aplicação.

Porém, ao tentar ler os dados de um soquete em que a outra aplicação não
enviou nenhum dado, apenas sente e espere. Se os programas do soquete simplesmente
esperarem por dados sem enviar nada, eles esperarão por muito lempo, então é uma
parte importante dos programas que se comunicam pela Internet é ter um protocolo.

Um protocolo é um conjunto de regras que determinam quem vai primeiro, o que ele
tem que fazer, quais as respostas para essa mensagem, quem envia depois, e assim 
por diante. Em resumo, cada uma das aplicações num soquete estão dançando e 
certificando de que nenhum dos dois pise no pé do outro.

Existem vários documentos que descrevem esses protocolos de rede. O protocolo HTTP 
é descrito no documento abaixo:

<https://www.w3.org/Protocols/rfc2616/rfc2616.txt>

Esse é um complexo e longo documento de 176 páginas com vários detalhes. Se você o
achar interessante, sinta-se livre para ler todo. Mas se você der uma olhada por 
volta da página 36 do RFC2616, você encontrará a sintáxe para a solicitação *GET*.
Para solicitar um documento de um servidor da Internet, nos conectamos com o
servidor `www.pr4e.org` pela porta 80 e então enviamos uma linha da forma:

`GET http://data.pr4e.org/romeo.txt HTTP/1.0 `

Em que o segundo parâmetro é a página que estamos solicitando e ainda enviamos uma
linha em branco. O servidor responderá com uma informação de cabeçalho sobre o
documento e a linha em branco seguida do conteúdo do documento.

O navegador da internet mais simples do mundo
-----------------------------------------------

Talvez a forma mais fácil de mostrar como o protocolo HTTP funciona é
escrevendo um simples programa em Python que faz conexão com um servidor 
da internet e segue as regras do protocolo HTTP para solicitar um documento
e mostrar em tela o que o servidor enviou como reposta.

\VerbatimInput{../code3/socket1.py}

Primeiramente o programa faz conexão com a porta 80 no servidor 
[www.py4e.com](http://www.py4e.com). Enquanto nosso programa está sendo o
"navegador da internet", o protocolo HTTP diz que precimaos enviar o 
comando *GET* seguido de uma linha em branco. `\r\n` significa um
FDL (final de linha), então `\r\n\r\n` significa nada entre dois FDL em 
sequência. Isso é equivalente a uma linha em branco.

![A Socket Connection](height=2.0in@../images/socket)

Tendo enviado a linha em branco, nós escrevemos um laço que recebe dados 
em pedaços de 512 caracteres do soquete e mostra na tela até não haver
mais dados para ler (i.e., the recv() retorna uma string nula)

O programa produz a seguinte saída:

~~~~
HTTP/1.1 200 OK
Date: Wed, 11 Apr 2018 18:52:55 GMT
Server: Apache/2.4.7 (Ubuntu)
Last-Modified: Sat, 13 May 2017 11:22:22 GMT
ETag: "a7-54f6609245537"
Accept-Ranges: bytes
Content-Length: 167
Cache-Control: max-age=0, no-cache, no-store, must-revalidate
Pragma: no-cache
Expires: Wed, 11 Jan 1984 05:00:00 GMT
Connection: close
Content-Type: text/plain

Mas, sutil! que luz ultrapassa aquela janela?
Esse é o leste, e Julieta é o sol.
Ascenda, honesto sol, e acabe com a invejosa lua,
Que já está doente e pálida com o luto,
~~~~

A saída começa com um cabeçalho que o servidor envia para descrever
o documento. Por exemplo, o cabeçalho `Content-Type` (tipo de conteúdo) 
indica que o documento é um texto simples (`text/plain`).

Após o servidor enviar o cabeçalho, ele adiciona uma linha em branco
para indicar o final do cabeçalho e então envia os dados do arquivo 
*romeo.txt*.

Esse exemplo mostra como fazer uma conexão de rede de baixo nível
com soquetes. Soquetes podem ser usados para comunicações com 
um servidor na internet ou de e-mail ou qualquer outro tipo de servidor. 
O necessário é encontrar o documento que descreve o protocolo e escrever 
um código que envie e receba os dados de acordo com o protocolo.

Entretanto, já que o protocolo mais comum é o HTTP, Python tem uma 
biblioteca, especificamente criada que suporta o protocolo HTTP para 
recuperação de documentos e dados pela internet.

Um dos requisitos para o uso de protocolo HTTP é a necessidade de 
enviar e receber dados como objetos em *bytes*, ao invés de *strings*. 
No exemplo seguinte, os métodos `encode()` e `decode()` convertem 
*strings* em  objetos em *bytes* e vice versa.

O exemplo seguinte utiliza a notação `b''` para especificar a variável 
que precisa ser armazenada como um objeto em *bytes*. `encode()` e
`b''` são equivalentes.

~~~~
>>> b'Hello world'
b'Hello world'
>>> 'Hello world'.encode()
b'Hello world'
~~~~

Recuperando uma imagem sobre http
-----------------------------

\index{urllib!image}
\index{image!jpg}
\index{jpg}

No exemplo acima, recuperamos um arquivo de texto de plano que tinha uma nova linha no arquivo e simplesmente copiamos os dados para a tela conforme o programa foi executado. Podemos usar um programa semelhante para recuperar uma imagem através do uso de http. em vez de copiar os dados para a tela como o programa é executado, nós acumulamos os dados em uma seqüência de caracteres, aparar os cabeçalhos e, em seguida, salvar os dados de imagem em um arquivo da seguinte maneira:

\VerbatimInput{../code3/urljpeg.py}

Quando o programa é executado, ele produz a seguinte saída:

~~~~
$ python urljpeg.py
5120 5120
5120 10240
4240 14480
5120 19600
...
5120 214000
3200 217200
5120 222320
5120 227440
3167 230607
Header length 393
HTTP/1.1 200 OK
Date: Wed, 11 Apr 2018 18:54:09 GMT
Server: Apache/2.4.7 (Ubuntu)
Last-Modified: Mon, 15 May 2017 12:27:40 GMT
ETag: "38342-54f8f2e5b6277"
Accept-Ranges: bytes
Content-Length: 230210
Vary: Accept-Encoding
Cache-Control: max-age=0, no-cache, no-store, must-revalidate
Pragma: no-cache
Expires: Wed, 11 Jan 1984 05:00:00 GMT
Connection: close
Content-Type: image/jpeg
~~~~

Você pode ver que para este URL, o cabeçalho do content-Type indica que o corpo do documento é na imagem(`image/jpeg`). uma vez que o programa for concluído, você pode visualizar os dados da imagem abrindo o arquivo stuff. jpg em um visualizador de imagens.

Como o programa é executado, você pode ver que não obtemos 5120 caracteres cada vez que chamamos o método `recv()`. obtemos tantos caracteres como foram transferidos através da rede para nós pelo servidor Web no momento em que chamamos `recv()`. Neste exemplo, nós  obtemos tão poucos como 3200 caracteres cada vez que solicitar até 5120 caracteres de dados.

Seus resultados podem ser diferente dependendo de sua velocidade de rede. Observe também que na última chamada para `recv()` obtemos 3167 bytes, que é o fim do fluxo, e na próxima chamada para `recv()`obtemos um comprimento zero que nos diz que o serviço chamou `close()` em seu final do soquete e não há mais dados próximos.

\index{time}
\index{time.sleep} 

podemos retardar nossas sucessivas chamadas de `recv()` descomentando a chamada para o `time.sleep()`. Desta forma, esperamos um quarto de segundo após cada chamada para que o servidor pode "chegar à frente" de nós e enviar mais dados para nós antes de chamar `recv()` novamente. com o atraso, no lugar o programa executa como segue:

~~~~
$ python urljpeg.py
5120 5120
5120 10240
5120 15360
...
5120 225280
5120 230400
207 230607
Header length 393
HTTP/1.1 200 OK
Date: Wed, 11 Apr 2018 21:42:08 GMT
Server: Apache/2.4.7 (Ubuntu)
Last-Modified: Mon, 15 May 2017 12:27:40 GMT
ETag: "38342-54f8f2e5b6277"
Accept-Ranges: bytes
Content-Length: 230210
Vary: Accept-Encoding
Cache-Control: max-age=0, no-cache, no-store, must-revalidate
Pragma: no-cache
Expires: Wed, 11 Jan 1984 05:00:00 GMT
Connection: close
Content-Type: image/jpeg
~~~~

Agora, além da primeira e última chamadas para `recv()`, agora temos 5120 caracteres cada vez que pedimos novos dados. 

Há como buffer entre o servidor que faz solicitações `send()` e nosso aplicativo fazendo solicitações`recv()`. Quando executamos o programa com o atraso no lugar, em algum momento o servidor pode encher o buffer no soquete e ser forçado a pausar até que o nosso programa começa a esvaziar o buffer. a pausa do aplicativo de envio ou o aplicativo de recebimento é chamado de "controle de fluxo"

\index{flow control} 

Retrieving web pages with `urllib`
---------------------------------------------

While we can manually send and receive data over HTTP using the socket
library, there is a much simpler way to perform this common task in
Python by using the `urllib` library.

Using `urllib`, you can treat a web page much like a file.
You simply indicate which web page you would like to retrieve and
`urllib` handles all of the HTTP protocol and header details.

The equivalent code to read the *romeo.txt* file from the web
using `urllib` is as follows:

\VerbatimInput{../code3/urllib1.py}

Once the web page has been opened with `urllib.urlopen`, we
can treat it like a file and read through it using a `for`
loop.

When the program runs, we only see the output of the contents of the
file. The headers are still sent, but the `urllib` code
consumes the headers and only returns the data to us.

~~~~
But soft what light through yonder window breaks
It is the east and Juliet is the sun
Arise fair sun and kill the envious moon
Who is already sick and pale with grief
~~~~

As an example, we can write a program to retrieve the data for
`romeo.txt` and compute the frequency of each word in the
file as follows:

\VerbatimInput{../code3/urlwords.py}

Again, once we have opened the web page, we can read it like a local
file.

Reading binary files using `urllib`
-----------------------------------

Sometimes you want to retrieve a non-text (or binary) file such as an
image or video file. The data in these files is generally not useful to
print out, but you can easily make a copy of a URL to a local file on
your hard disk using `urllib`.

\index{binary file}

The pattern is to open the URL and use `read` to download the
entire contents of the document into a string variable
(`img`) then write that information to a local file as
follows:

\VerbatimInput{../code3/curl1.py}

This program reads all of the data in at once across the network and
stores it in the variable `img` in the main memory of your
computer, then opens the file `cover.jpg` and writes the data
out to your disk. The `wb` argument for `open()` opens a binary file
for writing only. This program will work if the size of the file is less than
the size of the memory of your computer.

However if this is a large audio or video file, this program may crash
or at least run extremely slowly when your computer runs out of memory.
In order to avoid running out of memory, we retrieve the data in blocks
(or buffers) and then write each block to your disk before retrieving
the next block. This way the program can read any size file without
using up all of the memory you have in your computer.

\VerbatimInput{../code3/curl2.py}

In this example, we read only 100,000 characters at a time and then
write those characters to the `cover.jpg` file before
retrieving the next 100,000 characters of data from the web.

This program runs as follows:

~~~~
python curl2.py
230210 characters copied.
~~~~

Parsing HTML and scraping the web
---------------------------------

\index{web!scraping}
\index{parsing HTML}

One of the common uses of the `urllib` capability in Python
is to *scrape* the web. Web scraping is when we write a
program that pretends to be a web browser and retrieves pages, then
examines the data in those pages looking for patterns.

As an example, a search engine such as Google will look at the source of
one web page and extract the links to other pages and retrieve those
pages, extracting links, and so on. Using this technique, Google
*spiders* its way through nearly all of the pages on the
web.

Google also uses the frequency of links from pages it finds to a
particular page as one measure of how "important" a page is and how high
the page should appear in its search results.

Parsing HTML using regular expressions
--------------------------------------

One simple way to parse HTML is to use regular expressions to repeatedly
search for and extract substrings that match a particular pattern.

Here is a simple web page:

~~~~ {.html}
<h1>The First Page</h1>
<p>
If you like, you can switch to the
<a href="http://www.dr-chuck.com/page2.htm">
Second Page</a>.
</p>
~~~~

We can construct a well-formed regular expression to match and extract
the link values from the above text as follows:

~~~~
href="http[s]?://.+?"
~~~~

Our regular expression looks for strings that start with
"href=\"http://" or "href=\"https://", followed by one or more characters (`.+?`),
followed by another double quote. The question mark behind the `[s]?` indicates
to search for the string "http" followed by zero or one "s". 

The question mark added to the `.+?` indicates
that the match is to be done in a "non-greedy" fashion instead of a
"greedy" fashion. A non-greedy match tries to find the
*smallest* possible matching string and a greedy match
tries to find the *largest* possible matching string.

\index{greedy}
\index{non-greedy}

We add parentheses to our regular expression to indicate which part of
our matched string we would like to extract, and produce the following
program:

\index{regex!parentheses}
\index{parentheses!regular expression}

\VerbatimInput{../code3/urlregex.py}

The `ssl` library allows this program to access web sites that strictly
enforce HTTPS. The `read` method returns HTML source code as a bytes object 
instead of returning an HTTPResponse object. The `findall` regular expression
method will give us a list of all of the strings that match our
regular expression, returning only the link text between the double quotes.

When we run the program and input a URL, we get the following output:

~~~~
Enter - https://docs.python.org
https://docs.python.org/3/index.html
https://www.python.org/
https://docs.python.org/3.8/
https://docs.python.org/3.7/
https://docs.python.org/3.5/
https://docs.python.org/2.7/
https://www.python.org/doc/versions/
https://www.python.org/dev/peps/
https://wiki.python.org/moin/BeginnersGuide
https://wiki.python.org/moin/PythonBooks
https://www.python.org/doc/av/
https://www.python.org/
https://www.python.org/psf/donations/
http://sphinx.pocoo.org/
~~~~

Regular expressions work very nicely when your HTML is well formatted
and predictable. But since there are a lot of "broken" HTML pages out
there, a solution only using regular expressions might either miss some
valid links or end up with bad data.

This can be solved by using a robust HTML parsing library.

Análise de HTML usando BeautifulSoup
------------------------------------

\index{BeautifulSoup}

Mesmo que HTML se pareça com XML^[O formato XML é descrito no próximo capítulo.] e algumas páginas são realmente construídas para serem XML, a maioria das páginas HTML é geralmente quebrada de forma que causaria à um analista de XML a rejeição inteira da página por ter um formato inapropriado.

Há várias bibliotecas Python que podem te ajudar a analisar HTML e extrair dados das páginas.Cada uma das bibliotecas têm seus pontos fortes e fracos e você pode escolher uma baseado nas suas necessidades.

Por exemplo, vamos simplesmente analisar algumas entradas HTML e extrair links usando a biblioteca BeautifulSoup. BeautifulSoup tolera HTML altamente falho e ainda é muito simples extrair os dados que você precisa. Você pode baixar e instalar o BeautifulSoup em:

<https://pypi.python.org/pypi/beautifulsoup4>

Informações sobre a instalação do BeautifulSoup com a ferramenta pip do Python Package Index podem ser encontradas em:

<https://packaging.python.org/tutorials/installing-packages/>

Nós vamos usar `urllib` para ler a página e então usaremos `BeautifulSoup` para extrair os atributos `href` das tags âncora (`a`).

\index{BeautifulSoup}
\index{HTML}
\index{parsing!HTML}

\VerbatimInput{../code3/urllinks.py}

O programa solicita um endereço web, então abre a página web, lê os dados e os passa para o analisador BeautifulSoup e, em seguida, recupera todas as tags âncora e imprime o atributo href de cada tag.

Quando o programa é executado, ele produz a seguinte saída:

~~~~
Enter - https://docs.python.org
genindex.html
py-modindex.html
https://www.python.org/
#
whatsnew/3.6.html
whatsnew/index.html
tutorial/index.html
library/index.html
reference/index.html
using/index.html
howto/index.html
installing/index.html
distributing/index.html
extending/index.html
c-api/index.html
faq/index.html
py-modindex.html
genindex.html
glossary.html
search.html
contents.html
bugs.html
about.html
license.html
copyright.html
download.html
https://docs.python.org/3.8/
https://docs.python.org/3.7/
https://docs.python.org/3.5/
https://docs.python.org/2.7/
https://www.python.org/doc/versions/
https://www.python.org/dev/peps/
https://wiki.python.org/moin/BeginnersGuide
https://wiki.python.org/moin/PythonBooks
https://www.python.org/doc/av/
genindex.html
py-modindex.html
https://www.python.org/
#
copyright.html
https://www.python.org/psf/donations/
bugs.html
http://sphinx.pocoo.org/
~~~~

Essa lista é muito mais longa porque algumas tags âncora são caminhos relativos (e.g., tutorial/index.html) ou referências in-page (por exemplo, '#') que não incluem "http://" ou "https://", que era um requisito em nossa expressão regular.

Você também pode usar o BeautifulSoup para extrair várias partes de cada tag:

\VerbatimInput{../code3/urllink2.py}

~~~~
python urllink2.py
Enter - http://www.dr-chuck.com/page1.htm
TAG: <a href="http://www.dr-chuck.com/page2.htm">
Second Page</a>
URL: http://www.dr-chuck.com/page2.htm
Content: ['\nSecond Page']
Attrs: [('href', 'http://www.dr-chuck.com/page2.htm')]
~~~~

`html.parser` é o analisador de HTML incluído na biblioteca padrão do Pyhton 3. Informações sobre outros analisadores de HTML estão disponíveis em:

<http://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-a-parser>

Esses exemplos só começam a mostrar o poder do BeautifulSoup quando se trata de analisar HTML.

Seção bônus para usuários Unix / Linux
--------------------------------------

Se você tem um computador Linux, Unix, ou Macintosh, você provavelmente tem no seu sistema operacional comandos incorporados que recuperam tanto textos simples quanto arquivos binários usando o HTTP ou Protocolos de transferência de arquivos (FTP).Um Destes comandos é `curl`:

\index{curl}

~~~~ {.bash}
$ curl -O http://www.py4e.com/cover.jpg
~~~~

O comando `curl`é a abreviação para "copy URL" e, portanto, os dois exemplos listados anteriormente para recuperar arquivos binários com `urllib` são habilmente denominados `curl1.py` e `curl2.py` em www.py4e.com/code3, pois implementam uma funcionalidade semelhante ao comando `curl`. Há também um exemplo de programa `curl3.py` que faz essa tarefa de forma um pouco mais eficaz, caso você queira realmente usar esse padrão em um programa que está escrevendo.

Um outro comando que funciona de maneira muito similar é `wget`:

\index{wget}

~~~~ {.bash}
$ wget http://www.py4e.com/cover.jpg
~~~~

Ambos os comandos tornam a recuperação de páginas da Web e arquivos remotos uma tarefa simples.

Glossary
--------

BeautifulSoup
:   A Python library for parsing HTML documents and extracting data from
    HTML documents that compensates for most of the imperfections in the
    HTML that browsers generally ignore. You can download the
    BeautifulSoup code from [www.crummy.com](http://www.crummy.com).
\index{BeautifulSoup}

port
:   A number that generally indicates which application you are
    contacting when you make a socket connection to a server. As an
    example, web traffic usually uses port 80 while email traffic uses
    port 25.
\index{port}

scrape
:   When a program pretends to be a web browser and retrieves a web
    page, then looks at the web page content. Often programs are
    following the links in one page to find the next page so they can
    traverse a network of pages or a social network.
\index{socket}

socket
:   A network connection between two applications where the applications
    can send and receive data in either direction.
\index{socket}

spider
:   The act of a web search engine retrieving a page and then all the
    pages linked from a page and so on until they have nearly all of the
    pages on the Internet which they use to build their search index.
\index{spider}

Exercises
---------

**Exercise 1: Change the socket program `socket1.py` to prompt
the user for the URL so it can read any web page. You can use
`split('/')` to break the URL into its component parts so you
can extract the host name for the socket `connect` call. Add
error checking using `try` and `except` to handle
the condition where the user enters an improperly formatted or
non-existent URL.**

**Exercise 2: Change your socket program so that it counts the number of
characters it has received and stops displaying any text after it has
shown 3000 characters. The program should retrieve the entire document
and count the total number of characters and display the count of the
number of characters at the end of the document.**

**Exercise 3: Use `urllib` to replicate the previous exercise
of (1) retrieving the document from a URL, (2) displaying up to 3000
characters, and (3) counting the overall number of characters in the
document. Don't worry about the headers for this exercise, simply show
the first 3000 characters of the document contents.**

**Exercise 4: Change the `urllinks.py` program to extract and
count paragraph (p) tags from the retrieved HTML document and display
the count of the paragraphs as the output of your program. Do not
display the paragraph text, only count them. Test your program on
several small web pages as well as some larger web pages.**

**Exercise 5: (Advanced) Change the socket program so that it only shows
data after the headers and a blank line have been received. Remember
that `recv` receives characters (newlines and all), not lines.**

